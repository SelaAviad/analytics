{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20181111114401-0000\n"
                }
            ], 
            "source": "# MSGHUB-ioti-ws-staging-WS Staging-test-Eygv\n# Location: US South  Org: ioti-ws-staging  Space: prod\nstaging_hub_cred = {\n  \"instance_id\": \"25ae57ad-cc28-4b27-ab63-dd9e9c2fb56e\",\n  \"mqlight_lookup_url\": \"https://mqlight-lookup-prod02.messagehub.services.us-south.bluemix.net/Lookup?serviceId=25ae57ad-cc28-4b27-ab63-dd9e9c2fb56e\",\n  \"api_key\": \"IF62jFPaG1k8Q5fgs8n5zYSXNNPn8yoXzI6PNbuyW2G1uZk1\",\n  \"kafka_admin_url\": \"https://kafka-admin-prod02.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_rest_url\": \"https://kafka-rest-prod02.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_brokers_sasl\": [\n    \"kafka04-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka05-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka03-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka01-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka02-prod02.messagehub.services.us-south.bluemix.net:9093\"\n  ],\n  \"user\": \"IF62jFPaG1k8Q5fg\",\n  \"password\": \"s8n5zYSXNNPn8yoXzI6PNbuyW2G1uZk1\"\n}\n\n# learn-credential \nstaging_hub_cred_learn = {\n  \"instance_id\": \"25ae57ad-cc28-4b27-ab63-dd9e9c2fb56e\",\n  \"mqlight_lookup_url\": \"https://mqlight-lookup-prod02.messagehub.services.us-south.bluemix.net/Lookup?serviceId=25ae57ad-cc28-4b27-ab63-dd9e9c2fb56e\",\n  \"api_key\": \"ceLWpH1rkYZuI7xNsK8J5HQmMKhoYRIAY0PBCFE6I719KjlW\",\n  \"kafka_admin_url\": \"https://kafka-admin-prod02.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_rest_url\": \"https://kafka-rest-prod02.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_brokers_sasl\": [\n    \"kafka04-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka01-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka02-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka05-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka03-prod02.messagehub.services.us-south.bluemix.net:9093\"\n  ],\n  \"user\": \"ceLWpH1rkYZuI7xN\",\n  \"password\": \"sK8J5HQmMKhoYRIAY0PBCFE6I719KjlW\"\n}\n\n# MSGHUB-ioti-ws-demo-IoT4I Worker Safety -test\n# Location: US South  Org: ioti-worker-safety  Space: prod\ndemo_hub_cred = {\n  \"instance_id\": \"5b780d1a-223f-4f34-8c47-68c9adbbe1e1\",\n  \"mqlight_lookup_url\": \"https://mqlight-lookup-prod01.messagehub.services.us-south.bluemix.net/Lookup?serviceId=5b780d1a-223f-4f34-8c47-68c9adbbe1e1\",\n  \"api_key\": \"hr37zRU3s43EnvMnAXPnknUfP6nSbBg493PUfdW26JvSl2Mv\",\n  \"kafka_admin_url\": \"https://kafka-admin-prod01.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_rest_url\": \"https://kafka-rest-prod01.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_brokers_sasl\": [\n    \"kafka04-prod01.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka05-prod01.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka02-prod01.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka03-prod01.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka01-prod01.messagehub.services.us-south.bluemix.net:9093\"\n  ],\n  \"user\": \"hr37zRU3s43EnvMn\",\n  \"password\": \"AXPnknUfP6nSbBg493PUfdW26JvSl2Mv\"\n}\n\nmessage_hub_credentials = staging_hub_cred_learn "
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting confluent-kafka\n  Downloading https://files.pythonhosted.org/packages/30/7b/11880d74f6af94729fa794292127e9df44d3398868a03f0b980fdb4b693a/confluent_kafka-0.11.6-cp35-cp35m-manylinux1_x86_64.whl (3.9MB)\n\u001b[K    100% |################################| 3.9MB 224kB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: confluent-kafka\nSuccessfully installed confluent-kafka-0.11.6\n"
                }
            ], 
            "source": "!pip install confluent-kafka"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from random import random\nfrom datetime import datetime, timezone\nclass Device(object):\n    def __init__(self, name, latitude, longitude, timezone):\n        self.id = name\n        self.latitude = latitude\n        self.longitude = longitude\n        self.tz = timezone\n        self.temp = random() * 50\n        self.humidity = 50*random() + 50\n        self.baromin = 75 *random()\n    def getrain(self, humidity):\n        if (humidity > 75.0 or self.temp > 30):\n            return humidity * (0.001 * random())\n    def getReadingAsJSON(self):\n        humidity = (self.humidity * 0.98) + random()\n        reading =   {\n            \"id\": self.id,\n            \"tz\": self.tz,\n            \"dateutc\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"temperature\": (self.temp * 0.95) + random(),\n            \"baromin\": (self.baromin * 0.95) + random(),\n            \"humidity\": humidity,\n            \"rainin\": self.getrain(humidity),\n            \"timestamp\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n          }\n        return reading\n\ndef list_as_str(lizt):\n    return \",\".join(lizt)\n\n\ndef generate_data(duration=15):\n    sample_data = [(\"IALBERTA384\",\"America/Edmonton\",51.07976532,-115.33161163),\n            (\"IANDALUC208\", \"Europe/Madrid\", 37.62454224, -2.70604014), \n            (\"IANSEROY2\", \"Indian/Mahe\", -4.74099493,55.51583862), \n            (\"I1189\",\"Asia/Yekaterinburg\",57.15063477, 65.56357574)]\n    devices = []\n    for sample in sample_data:\n        name, timezone, latitude, longitude = sample\n        devices.append(Device(name,latitude,longitude, timezone))\n    \n    duration = duration * 60\n    if (duration is 0):\n        print(\"Will produce data indefinitely. Click Kernel >Interrupt Kernel to stop producing data.\")\n    count = 0\n    while count < duration or duration is 0:\n        for device in devices:\n            reading = device.getReadingAsJSON()\n            yield reading\n        time.sleep(1.0)\n        count = count + 1"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\nConsumer Config {'sasl.mechanisms': 'PLAIN', 'ssl.ca.location': '/etc/ssl/certs', 'sasl.username': 'token', 'bootstrap.servers': 'kafka04-prod02.messagehub.services.us-south.bluemix.net:9093,kafka01-prod02.messagehub.services.us-south.bluemix.net:9093,kafka02-prod02.messagehub.services.us-south.bluemix.net:9093,kafka05-prod02.messagehub.services.us-south.bluemix.net:9093,kafka03-prod02.messagehub.services.us-south.bluemix.net:9093', 'debug': 'all', 'sasl.password': 'ceLWpH1rkYZuI7xNsK8J5HQmMKhoYRIAY0PBCFE6I719KjlW', 'client.id': 'learn-sample-generator-producer', 'broker.version.fallback': '0.10.2.1', 'security.protocol': 'SASL_SSL', 'api.version.request': True}\nSun Nov 11 11:45:00 2018: Sending data to Message Hub for 3 minutes\nStarting job # 0 in a separate thread.\n"
                }, 
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<BackgroundJob #0: <function start at 0x7fe911032620>>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "from confluent_kafka import Producer\nimport time\nfrom datetime import datetime\nimport json\n\nTOPIC = \"learn-demo\"\nDURATION_IN_MINUTES = 3\n\nmhub_config  = {\n    'debug': 'all',\n    'security.protocol': 'SASL_SSL',\n    'ssl.ca.location': '/etc/ssl/certs',\n    'sasl.mechanisms': 'PLAIN',\n    'sasl.password': message_hub_credentials[\"api_key\"],\n    'bootstrap.servers': list_as_str(message_hub_credentials[\"kafka_brokers_sasl\"]),                               \n    'sasl.username': 'token',\n    'api.version.request': True,\n    'broker.version.fallback': '0.10.2.1',\n    'client.id': 'learn-sample-generator-producer'\n}\n\nprint('\\nConsumer Config {}'.format(mhub_config))\n\ndef delivery_report(err, msg):\n    if err is not None:\n        print('Message delivery failed: {}'.format(err))\n    else:        \n        print('Message delivered to topic: {} partition: [{}]  offset: {}'.format(msg.topic(), msg.partition()), msg.offset())\n                                    \ndef start():\n    counter=0;    \n    producer = Producer(**mhub_config)\n    for reading in generate_data(DURATION_IN_MINUTES):        \n        #message = 'This is a test message #{0}'.format(counter)\n        producer.poll(0)\n        #producer.produce(TOPIC, value=message, callback=delivery_report)\n        producer.produce(TOPIC, value=json.dumps(reading).encode('utf-8'), callback=delivery_report)\n        counter += 1\n    producer.flush()\n    print(\"Sending Events Complete\")\n\n\n            \nfrom IPython.lib import backgroundjobs as bg\n\nif DURATION_IN_MINUTES > 0:\n    print(datetime.now().ctime() +\": Sending data to Message Hub for %d minutes\" % DURATION_IN_MINUTES)\n\nproduce_jobs = bg.BackgroundJobManager()\nproduce_jobs.new(start)\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from confluent_kafka import Consumer, KafkaError\n\nmhub_consumer_config = {\n    'bootstrap.servers': list_as_str(message_hub_credentials[\"kafka_brokers_sasl\"]),\n    'group.id': 'mygroup',\n    'security.protocol': 'SASL_SSL',\n    'sasl.mechanisms': 'PLAIN',\n    'sasl.password': message_hub_credentials[\"api_key\"],\n    'bootstrap.servers': list_as_str(message_hub_credentials[\"kafka_brokers_sasl\"]),                               \n    'sasl.username': 'token',\n    'auto.offset.reset': 'earliest',\n    'client.id': 'learn-sample-generator-consumer'\n}\n\n\n# 'fetch.message.max.bytes': 125829120,\n#     'fetch.wait.max.ms': 1000,\n#     'queued.min.messages': 5,\n#     'session.timeout.ms': 299000,\n#     'enable.auto.commit': 'false',\n#     'receive.message.max.bytes': 603316706,\n#     'default.topic.config': {\n#         'auto.offset.reset': 'earliest'\n#     }\n\nc = Consumer(mhub_consumer_config)\nprint(\"Subscribing Topic {0}\".format(TOPIC))\nc.subscribe([TOPIC])\n\nfor i in range(10): #only print the first 10 messages.\n    message = c.poll(timeout=2.0)\n\n    if message is not None:\n        if message.error():\n            error = message.error()\n            if error.code() == KafkaError._PARTITION_EOF :\n                print(\"No more messages. Is data being sent to the %s topic? \" % TOPIC) \n                break\n            else:\n                print(\"Error: \" + error.str())\n                break\n        else:            \n            print(\"Incoming data: %s\" % message.value().decode(\"utf-8\"))   \n\n#         if msg is None:\n#             continue\n#         if msg.error():\n#             if msg.error().code() == KafkaError._PARTITION_EOF:\n#                 continue\n#             else:\n#                 print(msg.error())\n#                 # break\n#         print('Received message: {}'.format(msg.value().decode('utf-8')))\n\nprint('Closing Consumer')\nc.close()\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}