{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<div style=\"background:#F5F7FA; height:100px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Want to do more?</span><span style=\"border: 1px solid #3d70b2;padding: 15px;float:right;margin-right:40px; color:#3d70b2; \"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n<span style=\"color:#5A6872;\"> Try out this notebook with your free trial of IBM Watson Studio.</span>\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n# Ingest data from Message Hub in a streams flow\n\n\nIf your organization uses Message Hub to communicate between applications, you can easily ingest streaming data from Message Hub for analysis in real time by using IBM Streams Designer. Streams Designer is a <a href=\"https://developer.ibm.com/streamsdev/2017/11/28/quickly-create-streams-applications-using-new-streams-designer/\" target=\"_blank\" rel=\"noopener noreferrer\">web based graphical IDE</a> to help you create streaming analytics applications without having to write a lot of code. Applications created with Streams Designer are called *flows*. This tutorial will show you how to create a streams flow that uses streaming data from Message Hub. The final result is shown below.\n\n![mhub flow](https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2018/01/messagehubflow.gif)\nThis tutorial builds on the Data Historian example streams flow. That application uses sample data as its data source. The sample data is a stream of readings from weather stations (temperature, amount of rain, etc.) and is used to compute the average of those readings in the last hour. \n\nIn this tutorial, you'll modify that example to use Message Hub as its data source.\n\nThis notebook runs under Python 3.5 with Spark 2.1.\n\n\n## Table of Contents\n1. [Prerequisites](#prereq)\n1. [Set up a Message Hub instance](#step2)\n    <br/>2.1 [Create a Message Hub instance](#step21)\n    <br/>2.2 [Create a topic](#step22)\n1. [Publish data to the Message Hub service](#step3)\n    <br/>3.1 [Get your Message Hub credentials](#step31)\n    <br/>3.2 [Install Confluent\u2019s Apache Kafka Python client](#step32)\n    <br/>3.3 [Generate sample data](#step33)\n    <br/>3.4 [Start publishing the sample data to Message Hub](#step34)\n1. [Use the data from Message Hub in your streams flow](#step4)\n1. [Troubleshooting](#step5)\n1. [Next steps - Send data to Message Hub from your streams flow](#next)\n\n\n<a id=\"prereq\"></a>\n## Prerequisites\n\nSince the goal of this notebook is to modify the Data Historian example flow to use Message Hub as its data source, you need to import the Data Historian example flow. If you already have the flow imported in a project, skip this step.\nOtherwise, [watch this video](https://www.youtube.com/watch?v=rCNgJopanrY)  or follow the instructions below to import the Data Historian example flow.\n\n1. Go to **Tools** > **Streams Designer**, or from a project, click **Add to project** > **Streams flow.**\n1. Select **From example**. Choose a Streaming Analytics service, or create one if prompted to do so.  \n1. Click **Data Historian Example**. \n1. Select a connection to Cloud Object Storage. \n1. Under **File path**, click the slider button to select a bucket from your Object Storage instance. Enter a file name after the bucket name, like `/mybucket/data_historian_results_%TIME.txt`.\n1. Click **Create**. After the project is created, click *Run* to start the flow.\n1. Verify that data is being generated by logging into your Cloud Object Storage service to view the results file.\n\n\nYou can learn more about this example [here](https://dataplatform.ibm.com/docs/content/streaming-pipelines/data_historian_example_pipeline.html?audience=wdp&context=analytics&linkInPage=true).\n\n\n<a id=\"step2\"></a>\n## 2. Set up your Message Hub instance\n\n<a id=\"step21\"></a>\n\n### 2.1 Create a Message Hub instance\n\n\nIf you do not already have an instance of Message Hub, you must create one. You can do so from any page in the Watson Studio.   You must have already added a credit card to your IBM Cloud account to have access to Message Hub service.\n - Select **Data Services** from the toolbar, and then click **Services**. \n    ![Services page](https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/11/wdp-service.png)\n - Click **Create new** in the top right.\n - Choose **Message Hub** from the list of services that appear.\n - Click **Standard** to select the Standard pricing plan, and then click **Create**.\n - In the Confirm Creation dialog box that appears, change the Service Name if you wish.\n - Your service will be created, and then you will be returned to the Services page.\n\n\n<a id=\"step22\"></a>\n\n### Create a topic\n- In the Services page, find your Message Hub service. In the context menu under **Actions**, click **Manage in IBM Cloud**. \n![Manage in Cloud](https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/11/manage.png)\n<br/>\n- The Manage page of the service opens in a new tab. \n- A list of topics will appear. Click the **+** sign under **Topics** to add a new topic called `temperature`, and then click **Create topic**. \n![Create new topic](https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/11/topic.png)\n<br/><br/>\n\n<a id=\"step3\"></a>\n\n## 3. Publish data to your Message Hub instance\nBefore you can use data *from* Message Hub in your streams flow, you'll need an application that is sending data *to* Message Hub.  The next few cells of this notebook will create a simple application that will generate and send data to Message Hub for use in your streams flow.\n\nThe application needs to be configured with credentials of your Message Hub service so that it can securely send data.\n\nSo, first you must retrieve credentials for your Message Hub instance.\n\n<a id=\"step31\"></a>\n\n### 3.1 Get your Message Hub service credentials\n- In the Manage page of your service, click **Service credentials**.\n- A set of service credentials should already be created. If not, click **New credential** and accept the defaults. \n- Click **View credentials**, and then click the copy button to copy them to the clipboard.\n![copy credentials](https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/11/copycredentials.png)\n\n- Paste them in the cell below where indicated.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"creds\"></a>\n#### Paste your Message Hub credentials here", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "message_hub_credentials = #PASTE CREDENTIALS HERE#"
        }, 
        {
            "source": "<a id=\"step32\"></a>\n### 3.2 Install Confluent\u2019s Apache Kafka Python client\nThis client contains the API used to communicate with Message Hub.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): confluent-kafka in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s623-f8b248de4f938e-3d6cff768616/.local/lib/python3.5/site-packages\r\n"
                }
            ], 
            "source": "!pip install confluent-kafka"
        }, 
        {
            "source": "<a id=\"step33\"></a>\n### 3.3 Generate sample data\n\nThe next cell will generate simulated data for 4 weather stations. Each weather station is represented by the `Device` class. The `Device` class generates JSON data in the same input schema that is used in the Data Historian example pipeline:\n```\n{\n\"id\": \"IALBERTA598\",\n\"tz\": \"America/Edmonton\",\n\"dateutc\": \"2017-01-24 05:03:50\",\n\"latitude\": 50.88381958,\n\"longitude\": -113.98414612,\n\"temperature\": \"16.399999618530273\",\n\"baromin\": 30.69109211987511,\n\"humidity\": 109.53158304521693,\n\"rainin\": 0,\n\"time_stamp\": \"2017-11-21 01:41:05\"\n}\n```\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from random import random\nfrom datetime import datetime, timezone\nclass Device(object):\n    def __init__(self, name, latitude, longitude, timezone):\n        self.id = name\n        self.latitude = latitude\n        self.longitude = longitude\n        self.tz = timezone\n        self.temp = random() * 50\n        self.humidity = 50*random() + 50\n        self.baromin = 75 *random()\n    def getrain(self, humidity):\n        if (humidity > 75.0 or self.temp > 30):\n            return humidity * (0.001 * random())\n    def getReadingAsJSON(self):\n        humidity = (self.humidity * 0.98) + random()\n        reading =   {\n            \"id\": self.id,\n            \"tz\": self.tz,\n            \"dateutc\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"temperature\": (self.temp * 0.95) + random(),\n            \"baromin\": (self.baromin * 0.95) + random(),\n            \"humidity\": humidity,\n            \"rainin\": self.getrain(humidity),\n            \"time_stamp\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n          }\n        return reading\n\ndef list_as_str(lizt):\n    return \",\".join(lizt)\n\n\ndef generate_data(duration=15):\n    sample_data = [(\"IALBERTA384\",\"America/Edmonton\",51.07976532,-115.33161163),\n            (\"IANDALUC208\", \"Europe/Madrid\", 37.62454224, -2.70604014), \n            (\"IANSEROY2\", \"Indian/Mahe\", -4.74099493,55.51583862), \n            (\"I1189\",\"Asia/Yekaterinburg\",57.15063477, 65.56357574)]\n    devices = []\n    for sample in sample_data:\n        name, timezone, latitude, longitude = sample\n        devices.append(Device(name,latitude,longitude, timezone))\n    \n    duration = duration * 60\n    if (duration is 0):\n        print(\"Will produce data indefinitely. Click Kernel >Interrupt Kernel to stop producing data.\")\n    count = 0\n    while count < duration or duration is 0:\n        for device in devices:\n            reading = device.getReadingAsJSON()\n            yield reading\n        time.sleep(1.0)\n        count = count + 1"
        }, 
        {
            "source": "<a id=\"step34\"></a>\n### 3.4 Start publishing  sample data to Message Hub\n\nWe use the service credentials you pasted earlier to create an instance of the `Producer` class and then publish a stream of weather station readings to the *temperature* topic. You can use the `TOPIC` variable to change the topic.\n\n\n*Note*: This cell will run for 20 minutes in a background job. If your streams flow stops receiving data, re-run the cell to keep generating data. Alternatively, change the `DURATION_IN_MINUTES` variable. You can set the `DURATION_IN_MINUTES` to a larger value, or to `0` to run this cell indefinitely.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tue Jan  9 16:46:39 2018: Sending data to Message Hub for 20 minutes\nStarting job # 0 in a separate thread.\n"
                }, 
                {
                    "execution_count": 3, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<BackgroundJob #0: <function start at 0x7fe38ebf7510>>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "from confluent_kafka import Producer\nimport time\nfrom datetime import datetime\nimport json\n\nTOPIC = \"temperature\"\nDURATION_IN_MINUTES = 20\n\nmhub_config  = {\n    'debug': 'msg',\n    'security.protocol': 'SASL_SSL',\n    'sasl.mechanisms': 'PLAIN',\n    'sasl.password': message_hub_credentials[\"password\"],\n   'bootstrap.servers': list_as_str(message_hub_credentials[\"kafka_brokers_sasl\"]),                               \n   'sasl.username': message_hub_credentials[\"user\"]\n}\n                                    \ndef start():\n    producer = Producer(**mhub_config)\n    for reading in generate_data(DURATION_IN_MINUTES):\n        producer.produce(TOPIC, value=json.dumps(reading).encode('utf-8'))\n    producer.flush()\n\n\n            \nfrom IPython.lib import backgroundjobs as bg\n\nif DURATION_IN_MINUTES > 0:\n    print(datetime.now().ctime() +\": Sending data to Message Hub for %d minutes\" % DURATION_IN_MINUTES)\n\njobs = bg.BackgroundJobManager()\njobs.new(start)"
        }, 
        {
            "source": "<a id=\"step4\"></a>\n\n## 4. Use the data from Message Hub in your streams flow\n\nNow that we have data being sent to the Message Hub instance, you can use it in your flow. If the flow is currently running, click the *stop* button to stop it, and then click *edit* to edit it in the canvas.\n\n\n\n### Add and configure the Message Hub operator \n\n- Drag the Message Hub operator from the Source list to the canvas.\n- Under **Connection**, click **Add connection**. In the Create Connection window, choose your service instance under **Your service instances in IBM Cloud**. The credentials will be populated in the dialog. Click **Create**.\n![Message Hub Connection dialog](https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/11/dialog.png)\n\n- Under **Topic**, the list will be populated with the topics you have created in Message Hub.  Select the *temperature* topic.\n- Select **Edit Schema**, and then click **Show Preview** to verify that the data from the notebook is being sent correctly. You should see a preview of the data. If you do not see any data, make sure that the service credentials and name match what you created earlier.  Also, verify that the previous cell is running.\n- Click **Detect schema** to automatically determine the output schema of the operator. Make sure the values are as follows:\n    - `id` : Text\n    - `tz`: Text\n    - `dateutc`: Date\n    - `latitude`: Number\n    - `longitude`: Number\n    - `temperature`: Number\n    - `baromin`: Number\n    - `rainin`:  Number\n    - `humidity` : Number\n    - `time_stamp`: Date\n   \n<img alt=\"Schema detection\" src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/11/schema.png\"></img>\n- Click **Save**, and then **Close** to close the Edit Schema window.\n- Connect the Message Hub operator to the Aggregation operator, and then delete the Sample Data operator.  Your graph should look like this:\n<img alt=\"flow diagram with message hub\" src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/12/finalflow1.png\"></img>\n- \u2022\tClick **Save**, and then click **Run** to start the streams flow. After it starts, you should see data flowing from Message Hub.\n\n<img alt=\"running streams flow\" src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/12/runningflow2.png\"></img>\n    \n    \n<a id=\"step5\"></a>\n## 5. Troubleshooting\n\nIf you get any errors, check the following points:\n- Your Streaming Analytics service is started.\n- Credentials are correctly copied from Message Hub and <a href=\"#creds\">pasted in the cell</a>.\n- Data is being sent to Message Hub, [check step 3.4](#step34).\nYou could modify the cell below to try to read from the `temperature` topic. Change `RESULTS_TOPIC_NAME` to `temperature`. If that cell prints data from Message Hub, then the problem you are having is likely a related to the configuration of your Streams flow.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"next\"></a>\n## 6. Next steps - Send data to Message Hub from your streams flow\nSee if you can change the Streams flow to send the results  *to* Message Hub instead of Object Storage. \nIf it is working correctly, the next cell will print out the results of the Aggregation operator.\n \n**Hint:** You will need to create a new topic in Message Hub. Call it `results`.\nYou'll also need the Message Hub target operator, which you will find in the *Target* category in the canvas.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Incoming data: { \"id\" : \"I1189\", \"tz\" : \"Asia/Yekaterinburg\", \"dateutc\" : \"2017-12-11T20:59:34\", \"time_stamp\" : \"2017-12-11T20:59:34\", \"longitude\" : 65.56357574, \"latitude\" : 57.15063477, \"rainin\" : 0, \"humidity\" : 49.2586993697516, \"baromin\" : 18.2346408820349, \"temperature\" : 0.0232874316157452 }\nIncoming data: { \"id\" : \"IALBERTA384\", \"tz\" : \"America/Edmonton\", \"dateutc\" : \"2017-12-11T20:59:36\", \"time_stamp\" : \"2017-12-11T20:59:36\", \"longitude\" : -115.33161163, \"latitude\" : 51.07976532, \"rainin\" : 0, \"humidity\" : 22.117086699143, \"baromin\" : 68.1079591166223, \"temperature\" : 0.0183134239080065 }\nIncoming data: { \"id\" : \"IANDALUC208\", \"tz\" : \"Europe/Madrid\", \"dateutc\" : \"2017-12-11T20:59:36\", \"time_stamp\" : \"2017-12-11T20:59:36\", \"longitude\" : -2.70604014, \"latitude\" : 37.62454224, \"rainin\" : 0, \"humidity\" : 25.9265311927769, \"baromin\" : 27.7391455939335, \"temperature\" : 0.0195746529946135 }\nIncoming data: { \"id\" : \"IANSEROY2\", \"tz\" : \"Indian/Mahe\", \"dateutc\" : \"2017-12-11T20:59:36\", \"time_stamp\" : \"2017-12-11T20:59:36\", \"longitude\" : 55.51583862, \"latitude\" : -4.74099493, \"rainin\" : 0, \"humidity\" : 81.9131858479234, \"baromin\" : 13.857782695754, \"temperature\" : 0.0528405408079244 }\nIncoming data: { \"id\" : \"I1189\", \"tz\" : \"Asia/Yekaterinburg\", \"dateutc\" : \"2017-12-11T20:59:36\", \"time_stamp\" : \"2017-12-11T20:59:36\", \"longitude\" : 65.56357574, \"latitude\" : 57.15063477, \"rainin\" : 0, \"humidity\" : 48.3786589338269, \"baromin\" : 18.5571523289841, \"temperature\" : 0.076790066048293 }\nIncoming data: { \"id\" : \"IALBERTA384\", \"tz\" : \"America/Edmonton\", \"dateutc\" : \"2017-12-11T20:59:38\", \"time_stamp\" : \"2017-12-11T20:59:38\", \"longitude\" : -115.33161163, \"latitude\" : 51.07976532, \"rainin\" : 0, \"humidity\" : 21.7735060513338, \"baromin\" : 67.9822640938181, \"temperature\" : 0.119233444854667 }\nIncoming data: { \"id\" : \"IANDALUC208\", \"tz\" : \"Europe/Madrid\", \"dateutc\" : \"2017-12-11T20:59:38\", \"time_stamp\" : \"2017-12-11T20:59:38\", \"longitude\" : -2.70604014, \"latitude\" : 37.62454224, \"rainin\" : 0, \"humidity\" : 25.4805076947617, \"baromin\" : 28.3991059646348, \"temperature\" : 0.415482454053425 }\nIncoming data: { \"id\" : \"IANSEROY2\", \"tz\" : \"Indian/Mahe\", \"dateutc\" : \"2017-12-11T20:59:38\", \"time_stamp\" : \"2017-12-11T20:59:38\", \"longitude\" : 55.51583862, \"latitude\" : -4.74099493, \"rainin\" : 0, \"humidity\" : 81.5506741343814, \"baromin\" : 13.3492433336868, \"temperature\" : 0.273526186452909 }\nIncoming data: { \"id\" : \"I1189\", \"tz\" : \"Asia/Yekaterinburg\", \"dateutc\" : \"2017-12-11T20:59:38\", \"time_stamp\" : \"2017-12-11T20:59:38\", \"longitude\" : 65.56357574, \"latitude\" : 57.15063477, \"rainin\" : 0, \"humidity\" : 48.6750845124346, \"baromin\" : 18.2073641964531, \"temperature\" : 0.0708896898852753 }\nIncoming data: { \"id\" : \"IALBERTA384\", \"tz\" : \"America/Edmonton\", \"dateutc\" : \"2017-12-11T20:59:40\", \"time_stamp\" : \"2017-12-11T20:59:40\", \"longitude\" : -115.33161163, \"latitude\" : 51.07976532, \"rainin\" : 0, \"humidity\" : 21.8635825021746, \"baromin\" : 68.1004201827024, \"temperature\" : 0.0145249467698025 }\n"
                }
            ], 
            "source": "from confluent_kafka import Consumer, KafkaError\n\nRESULTS_TOPIC_NAME = \"results\"\nmhub_config_consumer  = {\n     'group.id': 'mygroup',\n    'security.protocol': 'SASL_SSL',\n    'sasl.mechanisms': 'PLAIN',\n    'sasl.password': message_hub_credentials[\"password\"],\n   'bootstrap.servers': list_as_str(message_hub_credentials[\"kafka_brokers_sasl\"]),                               \n   'sasl.username': message_hub_credentials[\"user\"]\n}\nconsumer = Consumer(**mhub_config_consumer)\nconsumer.subscribe([RESULTS_TOPIC_NAME])\nfor i in range(10): #only print the first 10 messages.\n    message = consumer.poll(timeout=2.0)\n    \n    if message is not None:\n        if message.error():\n            error = message.error()\n            if error.code() == KafkaError._PARTITION_EOF :\n                print(\"No more messages. Is data being sent to the %s topic? \" % RESULTS_TOPIC_NAME) \n                break\n            else:\n                print(\"Error: \" + error.str())\n                break\n        else:\n            print(\"Incoming data: %s\" % message.value().decode(\"utf-8\"))        \n        \n        \nconsumer.close()"
        }, 
        {
            "source": "### More information\n- [Learn more about the other operators available in Streams Designer](https://dataplatform.ibm.com/docs/content/streaming-pipelines/creating-pipeline-manually.html?audience=wdp).\n- [Visit Streamsdev](https://developer.ibm.com/streamsdev) to learn more about Streams.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Author\n\n**Natasha D'Silva** is a software developer at IBM Canada who specializes in streaming technology and cloud solutions.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "Copyright \u00a9 IBM Corp. 2017, 2018. This notebook and its source code are released under the terms of the Apache 2.0 License.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}